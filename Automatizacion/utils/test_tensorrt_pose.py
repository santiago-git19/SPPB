#!/usr/bin/env python3
"""
Script de DiagnÃ³stico TensorRT Pose Processor
=============================================

Script para verificar que el sistema TensorRT funciona correctamente
antes de procesar videos o usar en producciÃ³n.

Ejecutar con:
    python test_tensorrt_pose.py

Autor: Sistema de IA
Fecha: 2025
"""

import os
import sys
import time
import numpy as np

# Agregar el directorio actual al path para importar
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from mediapipe_pose_proc import MediaPipePoseProcessor, TRT_AVAILABLE
except ImportError as e:
    print(f"âŒ Error importando MediaPipePoseProcessor: {e}")
    print("ğŸ’¡ AsegÃºrese de que mediapipe_pose_proc.py estÃ© en el mismo directorio")
    exit(1)

def main():
    """FunciÃ³n principal de diagnÃ³stico"""
    print("ğŸ”§ SCRIPT DE DIAGNÃ“STICO TENSORRT POSE PROCESSOR")
    print("=" * 60)
    
    # Verificar disponibilidad de TensorRT
    if not TRT_AVAILABLE:
        print("âŒ TensorRT no estÃ¡ disponible")
        print("ğŸ’¡ Instale TensorRT y PyCUDA para usar esta clase")
        print("\nğŸ“‹ InstalaciÃ³n requerida:")
        print("   1. TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/")
        print("   2. PyCUDA: pip install pycuda")
        exit(1)
    
    print("âœ… TensorRT y PyCUDA disponibles")
    
    # Buscar modelo TensorRT
    print("\nğŸ” Buscando modelo TensorRT...")
    
    # Rutas posibles del modelo
    possible_paths = [
        "pose_landmark_lite_fp16.engine",
        "../models/pose_landmark_lite_fp16.engine", 
        "models/pose_landmark_lite_fp16.engine",
        "Documentos/Trabajo/SPPB/Automatizacion/models/pose_landmark_lite_fp16.engine"
    ]
    
    model_path = None
    for path in possible_paths:
        if os.path.exists(path):
            model_path = path
            print(f"âœ… Modelo encontrado: {path}")
            break
    
    if model_path is None:
        print("âŒ No se encontrÃ³ el modelo pose_landmark_lite_fp16.engine")
        print("\nğŸ” Buscando cualquier modelo .engine...")
        
        # Buscar cualquier archivo .engine
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.endswith('.engine'):
                    full_path = os.path.join(root, file)
                    print(f"ğŸ“ Encontrado: {full_path}")
                    if model_path is None:
                        model_path = full_path
        
        if model_path is None:
            print("âŒ No se encontraron archivos .engine")
            print("ğŸ’¡ AsegÃºrese de tener el modelo pose_landmark_lite_fp16.engine")
            exit(1)
        else:
            print(f"ğŸ¯ Usando: {model_path}")
    
    # Crear procesador y ejecutar diagnÃ³sticos
    print(f"\nğŸš€ Inicializando procesador con modelo: {os.path.basename(model_path)}")
    
    try:
        processor = MediaPipePoseProcessor(
            model_path=model_path,
            input_width=256,
            input_height=256,
            confidence_threshold=0.5
        )
        
        # Ejecutar diagnÃ³sticos completos
        success = processor.run_diagnostics()
        
        # Resultados finales
        print("\n" + "ğŸ¯ RESULTADOS FINALES" + " " + "ğŸ¯")
        print("=" * 60)
        
        if success:
            print("ğŸ‰ Â¡EXCELENTE! El sistema estÃ¡ funcionando perfectamente")
            print("\nâœ… Puede proceder a:")
            print("   â€¢ Procesar videos con mediapipe_pose_proc.py")
            print("   â€¢ Integrar en sus aplicaciones")
            print("   â€¢ Usar en producciÃ³n")
            
            print("\nğŸ“Š InformaciÃ³n del sistema:")
            print(f"   â€¢ Modelo: {os.path.basename(model_path)}")
            print(f"   â€¢ Entrada: {processor.input_width}x{processor.input_height}")
            print(f"   â€¢ Confianza: {processor.confidence_threshold}")
            print(f"   â€¢ Keypoints: 33 (MediaPipe BlazePose)")
            
        else:
            print("âŒ Se detectaron problemas en el sistema")
            print("\nğŸ”§ Acciones recomendadas:")
            print("   1. Verificar que el modelo .engine sea compatible")
            print("   2. Regenerar el modelo en esta mÃ¡quina:")
            print("      trtexec --onnx=pose_landmark_lite.onnx --saveEngine=pose_landmark_lite_fp16.engine --fp16")
            print("   3. Verificar instalaciÃ³n de TensorRT y PyCUDA")
            print("   4. Comprobar drivers NVIDIA y CUDA")
            
        # Limpiar recursos
        processor.cleanup()
        
    except Exception as e:
        print(f"\nâŒ ERROR CRÃTICO: {e}")
        print("\nğŸ”§ Soluciones posibles:")
        print("   1. Verificar que CUDA estÃ© instalado y funcionando")
        print("   2. Verificar que los drivers NVIDIA estÃ©n actualizados")
        print("   3. Regenerar el modelo .engine en esta mÃ¡quina")
        print("   4. Verificar permisos de archivos")
        return False
    
    return success

def test_model_compatibility(model_path: str):
    """
    Prueba la compatibilidad del modelo con trtexec
    
    Args:
        model_path: Ruta al modelo .engine
    """
    print(f"\nğŸ§ª Probando compatibilidad del modelo con trtexec...")
    
    try:
        import subprocess
        
        # Ejecutar trtexec para probar el modelo
        cmd = f"trtexec --loadEngine={model_path} --iterations=1 --avgRuns=1"
        
        print(f"   ğŸ“ Ejecutando: {cmd}")
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            print("   âœ… Modelo compatible con TensorRT")
            
            # Extraer informaciÃ³n del modelo
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if 'Created input binding' in line:
                    print(f"   ğŸ“ {line.strip()}")
                elif 'Created output binding' in line:
                    print(f"   ğŸ“Š {line.strip()}")
                elif 'Average on' in line and 'GPU latency' in line:
                    print(f"   â±ï¸ {line.strip()}")
                    
        else:
            print("   âŒ Modelo incompatible o error en TensorRT")
            print(f"   ğŸ“ Error: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        print("   â° Timeout - El modelo tarda mucho en cargar")
    except FileNotFoundError:
        print("   âŒ trtexec no encontrado - TensorRT no instalado correctamente")
    except Exception as e:
        print(f"   âŒ Error probando modelo: {e}")

if __name__ == "__main__":
    print("ğŸš€ Iniciando diagnÃ³sticos del sistema TensorRT...")
    print(f"ğŸ“… Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ–¥ï¸ Directorio: {os.getcwd()}")
    
    success = main()
    
    print(f"\nâ° DiagnÃ³sticos completados: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    if success:
        exit(0)  # Ã‰xito
    else:
        exit(1)  # Error
