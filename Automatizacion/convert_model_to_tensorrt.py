#!/usr/bin/env python3
"""
Convertidor PyTorch a TensorRT - Jetson Nano Optimizado
=======================================================

Script para convertir modelos PyTorch a TensorRT con:
- Monitoreo completo de recursos durante la conversi√≥n
- Configuraci√≥n autom√°tica de swap
- Limitaci√≥n de CPU para evitar sobrecalentamiento
- Reportes de progreso detallados
- Manejo robusto de memoria

Autor: Sistema de IA
Fecha: 2025
"""

import json
import torch
import time
import sys
import os
import gc
import psutil
import subprocess
import threading
from pathlib import Path
import logging
from datetime import datetime
import trt_pose.models
import trt_pose.coco
import torch2trt

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Importar utilidades de Jetson
from utils.jetson_utils import (
    JetsonResourceMonitor, 
    JetsonSwapManager, 
    JetsonCPULimiter, 
    setup_jetson_optimizations
)

class TensorRTModelConverter:
    """Convertidor optimizado de PyTorch a TensorRT para Jetson Nano"""
    
    def __init__(self):
        # Configurar monitores de recursos
        self.resource_monitor = JetsonResourceMonitor(
            log_interval=15,  # M√°s frecuente durante conversi√≥n
            memory_threshold=80.0,  # M√°s conservador
            temperature_threshold=70.0  # M√°s conservador
        )
        self.swap_manager = JetsonSwapManager(swap_size_gb=4)  # M√°s swap para conversi√≥n
        self.cpu_limiter = JetsonCPULimiter()
        
        # Configuraci√≥n de rutas
        self.model_config = {
            'pytorch_model': '/home/mobilenet/Documentos/Trabajo/trt_pose/models/resnet18_baseline_att_224x224_A_epoch_249.pth',
            'topology_file': '/home/mobilenet/Documentos/Trabajo/trt_pose/tasks/human_pose/human_pose.json',
            'output_model': 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth',
            'backup_model': 'resnet18_baseline_att_224x224_A_epoch_249_trt_backup.pth'
        }
        
        # Configuraci√≥n de conversi√≥n optimizada para memoria limitada
        self.conversion_config = {
            'width': 224,
            'height': 224,
            'batch_size': 1,
            'fp16_mode': True,
            'max_workspace_size': 1 << 20,  # 1MB (ultra conservador para 930MB RAM)
            'strict_type_constraints': True,
            'int8_mode': False,  # FP16 es suficiente para Jetson Nano
            'minimum_segment_size': 3,  # Fusionar solo segmentos grandes
            'max_batch_size': 1,  # M√°ximo batch size
            'optimize_for_memory': True  # Priorizar memoria sobre velocidad
        }
        
        self._setup_alert_callbacks()
        
    def _setup_alert_callbacks(self):
        """Configura callbacks para alertas durante conversi√≥n"""
        def memory_alert(percent):
            logger.warning("üö® ALERTA MEMORIA: %.1f%% - Pausando conversi√≥n...", percent)
            self._emergency_memory_cleanup()
            time.sleep(5)  # Pausa m√°s larga durante conversi√≥n
            
        def temperature_alert(temp):
            logger.warning("üå°Ô∏è ALERTA TEMPERATURA: %.1f¬∞C - Pausando para enfriar...", temp)
            time.sleep(10)  # Pausa para enfriamiento
            
        def cpu_alert(percent):
            logger.warning("üíª ALERTA CPU: %.1f%% - Reduciendo carga...", percent)
            
        self.resource_monitor.add_callback('memory_alert', memory_alert)
        self.resource_monitor.add_callback('temperature_alert', temperature_alert)
        self.resource_monitor.add_callback('cpu_alert', cpu_alert)
        
    def _emergency_memory_cleanup(self):
        """Limpieza agresiva de memoria durante conversi√≥n"""
        logger.info("üßπ Liberando memoria...")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
    def setup_environment(self):
        """Configura el entorno optimizado para conversi√≥n"""
        logger.info("üöÄ Configurando entorno para conversi√≥n TensorRT...")
        
        # Configuraciones espec√≠ficas para conversi√≥n
        setup_jetson_optimizations()
        
        # Variables adicionales para conversi√≥n
        conversion_env = {
            'CUDA_VISIBLE_DEVICES': '0',
            'TRT_LOGGER_VERBOSITY': '1',  # M√°s verboso para conversi√≥n
            'CUDA_LAUNCH_BLOCKING': '1',  # Sincronizaci√≥n para debugging
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:32'  # Limitar fragmentaci√≥n
        }
        
        for key, value in conversion_env.items():
            os.environ[key] = value
            logger.info("‚úÖ %s=%s", key, value)
            
        # Configurar swap ampliado para conversi√≥n
        if not self.swap_manager.setup_swap():
            logger.warning("‚ö†Ô∏è No se pudo configurar swap autom√°ticamente")
            logger.info("üí° Para configurar manualmente:")
            logger.info("   sudo fallocate -l 4G /swapfile")
            logger.info("   sudo chmod 600 /swapfile") 
            logger.info("   sudo mkswap /swapfile")
            logger.info("   sudo swapon /swapfile")
            
        # Limitar a 1 core durante conversi√≥n para evitar sobrecalentamiento
        if not self.cpu_limiter.limit_cpu_cores([0]):
            logger.warning("‚ö†Ô∏è No se pudo limitar CPU")
            
        # Iniciar monitoreo intensivo
        self.resource_monitor.start_monitoring()
        
        logger.info("‚úÖ Entorno configurado para conversi√≥n")
        
    def verify_model_files(self):
        """Verifica que los archivos necesarios existan"""
        logger.info("üîç Verificando archivos del modelo...")
        
        pytorch_model = self.model_config['pytorch_model']
        topology_file = self.model_config['topology_file']
        
        if not os.path.exists(pytorch_model):
            logger.error("‚ùå Modelo PyTorch no encontrado: %s", pytorch_model)
            return False
            
        if not os.path.exists(topology_file):
            logger.error("‚ùå Archivo de topolog√≠a no encontrado: %s", topology_file)
            return False
            
        # Verificar tama√±o del modelo
        model_size = os.path.getsize(pytorch_model) / (1024**2)
        logger.info("‚úÖ Modelo PyTorch: %.1f MB", model_size)
        logger.info("‚úÖ Archivo de topolog√≠a encontrado")
        
        return True
        
    def load_model_configuration(self):
        """Carga la configuraci√≥n del modelo"""
        logger.info("üìã Cargando configuraci√≥n del modelo...")
        
        try:
            # Cargar topolog√≠a
            with open(self.model_config['topology_file'], 'r') as f:
                self.human_pose = json.load(f)
                
            self.topology = trt_pose.coco.coco_category_to_topology(self.human_pose)
            self.num_parts = len(self.human_pose['keypoints'])
            self.num_links = len(self.human_pose['skeleton'])
            
            logger.info("‚úÖ Partes del cuerpo: %d", self.num_parts)
            logger.info("‚úÖ Conexiones: %d", self.num_links)
            
            return True
            
        except Exception as e:
            logger.error("‚ùå Error cargando configuraci√≥n: %s", str(e))
            return False
            
    def create_pytorch_model(self):
        """Crea y carga el modelo PyTorch"""
        logger.info("üèóÔ∏è Creando modelo PyTorch...")
        
        try:
            # Verificar memoria antes de cargar modelo
            memory = psutil.virtual_memory()
            logger.info("Memoria disponible: %.1f GB", memory.available / (1024**3))
            
            if memory.available < 1024**3:  # Menos de 1GB
                logger.warning("‚ö†Ô∏è Poca memoria disponible, liberando recursos...")
                self._emergency_memory_cleanup()
                
            # Crear modelo
            self.model = trt_pose.models.resnet18_baseline_att(
                self.num_parts, 2 * self.num_links
            ).cuda().eval()
            
            logger.info("‚úÖ Modelo creado en GPU")
            
            # Cargar pesos
            logger.info("üì• Cargando pesos del modelo...")
            checkpoint = torch.load(self.model_config['pytorch_model'], map_location='cuda:0')
            self.model.load_state_dict(checkpoint)
            
            logger.info("‚úÖ Pesos cargados exitosamente")
            
            # Verificar memoria despu√©s de cargar
            memory = psutil.virtual_memory()
            logger.info("Memoria tras cargar modelo: %.1f GB disponible", memory.available / (1024**3))
            
            return True
            
        except Exception as e:
            logger.error("‚ùå Error creando modelo PyTorch: %s", str(e))
            return False
            
    def create_test_input(self):
        """Crea tensor de prueba para conversi√≥n"""
        logger.info("üéØ Creando tensor de entrada para conversi√≥n...")
        
        try:
            width = self.conversion_config['width']
            height = self.conversion_config['height']
            batch_size = self.conversion_config['batch_size']
            
            # Crear tensor de prueba
            self.test_input = torch.zeros(
                (batch_size, 3, height, width), 
                dtype=torch.float32,
                device='cuda:0'
            )
            
            logger.info("‚úÖ Tensor de entrada: %s", list(self.test_input.shape))
            
            # Verificar que el modelo funciona con la entrada
            logger.info("üß™ Probando inferencia PyTorch...")
            with torch.no_grad():
                output = self.model(self.test_input)
                logger.info("‚úÖ Inferencia PyTorch exitosa")
                logger.info("   Salida: %s", [list(o.shape) for o in output])
                
            return True
            
        except Exception as e:
            logger.error("‚ùå Error creando tensor de prueba: %s", str(e))
            return False
            
    def convert_to_tensorrt(self):
        """Realiza la conversi√≥n a TensorRT con fallback autom√°tico CPU"""
        logger.info("‚ö° Iniciando conversi√≥n PyTorch ‚Üí TensorRT...")
        
        # Diagn√≥stico inicial de memoria
        self._diagnose_memory_limitations()
        
        try:
            # Intentar conversi√≥n GPU primero
            logger.info("üéØ Intentando conversi√≥n GPU...")
            return self._convert_gpu()
            
        except (torch.cuda.OutOfMemoryError, RuntimeError) as e:
            if "out of memory" in str(e).lower():
                logger.warning("üíæ OOM en GPU detectado, usando fallback CPU...")
                logger.warning("   Esto usar√° swap efectivamente pero ser√° m√°s lento...")
                return self._convert_cpu_with_swap()
            else:
                logger.error("‚ùå Error no relacionado con memoria: %s", str(e))
                raise e
        except Exception as e:
            logger.error("‚ùå Error inesperado durante conversi√≥n: %s", str(e))
            return False
    
    def _diagnose_memory_limitations(self):
        """Diagn√≥stica limitaciones espec√≠ficas de memoria"""
        logger.info("üîç DIAGN√ìSTICO DE MEMORIA JETSON NANO:")
        
        # Memoria total del sistema
        mem_info = psutil.virtual_memory()
        total_mem_gb = mem_info.total / (1024**3)
        available_mem_gb = mem_info.available / (1024**3)
        logger.info(f"   üíæ RAM Total: {total_mem_gb:.1f} GB")
        logger.info(f"   üíæ RAM Disponible: {available_mem_gb:.1f} GB")
        
        # Memoria CUDA disponible
        if torch.cuda.is_available():
            total_cuda_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            allocated_cuda_gb = torch.cuda.memory_allocated(0) / (1024**3)
            cached_cuda_gb = torch.cuda.memory_reserved(0) / (1024**3)
            free_cuda_gb = total_cuda_gb - cached_cuda_gb
            
            logger.info(f"   üéÆ CUDA Total: {total_cuda_gb:.1f} GB")
            logger.info(f"   üéÆ CUDA Asignada: {allocated_cuda_gb:.1f} GB")
            logger.info(f"   üéÆ CUDA Cached: {cached_cuda_gb:.1f} GB")
            logger.info(f"   üéÆ CUDA Libre: {free_cuda_gb:.1f} GB")
        
        # Estado del swap
        swap_info = psutil.swap_memory()
        if swap_info.total > 0:
            swap_total_gb = swap_info.total / (1024**3)
            swap_used_gb = swap_info.used / (1024**3)
            swap_free_gb = (swap_info.total - swap_info.used) / (1024**3)
            logger.info(f"   üîÑ Swap Total: {swap_total_gb:.1f} GB")
            logger.info(f"   üîÑ Swap Usado: {swap_used_gb:.1f} GB")
            logger.info(f"   üîÑ Swap Libre: {swap_free_gb:.1f} GB")
        else:
            logger.warning("   ‚ö†Ô∏è Sin swap configurado")
        
        # Recomendar estrategia
        if available_mem_gb < 1.5:
            logger.warning("üí° MEMORIA BAJA: Recomendado usar conversi√≥n CPU con swap")
        elif free_cuda_gb < 0.5:
            logger.warning("üí° GPU MEMORY BAJA: Conversi√≥n puede fallar, CPU fallback disponible")
        else:
            logger.info("üí° Memoria aparenta ser suficiente para conversi√≥n GPU")
    
    def _convert_gpu(self):
        """Conversi√≥n est√°ndar usando GPU"""
        try:
            # Verificar recursos antes de conversi√≥n
            stats = self.resource_monitor.get_current_stats()
            memory_percent = stats.get('memory', {}).get('percent', 0)
            temp = stats.get('temperature', 0)
            
            logger.info("Estado inicial GPU:")
            logger.info("  Memoria: %.1f%%", memory_percent)
            logger.info("  Temperatura: %.1f¬∞C", temp or 0)
            
            if memory_percent > 75:
                logger.warning("‚ö†Ô∏è Memoria alta antes de conversi√≥n, liberando...")
                self._emergency_memory_cleanup()
                
            # Configurar par√°metros de conversi√≥n
            conversion_params = {
                'fp16_mode': self.conversion_config['fp16_mode'],
                'max_workspace_size': self.conversion_config['max_workspace_size'],
                'strict_type_constraints': self.conversion_config['strict_type_constraints']
            }
            
            logger.info("Par√°metros de conversi√≥n GPU:")
            for key, value in conversion_params.items():
                logger.info("  %s: %s", key, value)
                
            # Realizar conversi√≥n con monitoreo
            start_time = time.time()
            
            logger.info("üîÑ Ejecutando torch2trt en GPU...")
            logger.info("   Esto puede tomar 5-15 minutos en Jetson Nano...")
            
            # Thread para monitoreo durante conversi√≥n
            conversion_active = True
            
            def monitor_conversion():
                while conversion_active:
                    stats = self.resource_monitor.get_current_stats()
                    swap_info = psutil.swap_memory()
                    elapsed = time.time() - start_time
                    
                    logger.info("‚è±Ô∏è Conversi√≥n GPU (%.1f min) - Memoria: %.1f%% - Temp: %.1f¬∞C - Swap: %.1f%%",
                              elapsed/60, 
                              stats.get('memory', {}).get('percent', 0),
                              stats.get('temperature', 0) or 0,
                              swap_info.percent if swap_info.total > 0 else 0)
                    time.sleep(30)  # Log cada 30 segundos
                    
            monitor_thread = threading.Thread(target=monitor_conversion, daemon=True)
            monitor_thread.start()
            
            try:
                # Conversi√≥n principal
                self.model_trt = torch2trt.torch2trt(
                    self.model,
                    [self.test_input],
                    **conversion_params
                )
                conversion_active = False
                
                elapsed = time.time() - start_time
                logger.info("‚úÖ Conversi√≥n GPU completada en %.1f minutos", elapsed/60)
                
            except Exception as conversion_error:
                conversion_active = False
                raise conversion_error
                
            # Verificar el modelo convertido
            logger.info("üß™ Probando modelo TensorRT...")
            with torch.no_grad():
                trt_output = self.model_trt(self.test_input)
                logger.info("‚úÖ Inferencia TensorRT exitosa")
                
            return True
            
        except torch.cuda.OutOfMemoryError as e:
            logger.error("üí• GPU Out of Memory: %s", str(e))
            raise e
        except Exception as e:
            logger.error("‚ùå Error durante conversi√≥n GPU: %s", str(e))
            raise e
    
    def _convert_cpu_with_swap(self):
        """Conversi√≥n en CPU que S√ç usa swap efectivamente"""
        logger.info("üîÑ INICIANDO CONVERSI√ìN CPU (usa swap)...")
        logger.warning("   ‚è∞ Esto ser√° m√°s lento (15-30 min) pero m√°s estable")
        
        try:
            # Limpiar GPU memory primero
            self._emergency_memory_cleanup()
            
            # Mover modelo y datos a CPU
            logger.info("üì§ Moviendo modelo a CPU...")
            self.model = self.model.cpu()
            self.test_input = self.test_input.cpu()
            
            # M√°s limpieza despu√©s de mover a CPU
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            
            # Configurar par√°metros para CPU
            conversion_params = {
                'fp16_mode': False,  # CPU no soporta FP16
                'max_workspace_size': self.conversion_config['max_workspace_size'] * 2,  # M√°s workspace en CPU
                'strict_type_constraints': self.conversion_config['strict_type_constraints']
            }
            
            logger.info("Par√°metros de conversi√≥n CPU:")
            for key, value in conversion_params.items():
                logger.info("  %s: %s", key, value)
            
            # Monitoreo espec√≠fico para conversi√≥n CPU
            start_time = time.time()
            conversion_active = True
            
            def monitor_cpu_conversion():
                initial_swap = psutil.swap_memory().used
                while conversion_active:
                    stats = self.resource_monitor.get_current_stats()
                    swap_info = psutil.swap_memory()
                    elapsed = time.time() - start_time
                    swap_increase = (swap_info.used - initial_swap) / (1024**2)  # MB
                    
                    logger.info("‚è±Ô∏è Conversi√≥n CPU (%.1f min) - RAM: %.1f%% - Swap: %.1f%% (+%.0f MB) - Temp: %.1f¬∞C",
                              elapsed/60,
                              stats.get('memory', {}).get('percent', 0),
                              swap_info.percent if swap_info.total > 0 else 0,
                              swap_increase,
                              stats.get('temperature', 0) or 0)
                    
                    if swap_increase > 100:  # Si est√° usando swap significativamente
                        logger.info("‚úÖ Swap siendo usado efectivamente")
                    
                    time.sleep(45)  # M√°s frecuente para CPU
                    
            monitor_thread = threading.Thread(target=monitor_cpu_conversion, daemon=True)
            monitor_thread.start()
            
            # Conversi√≥n en CPU
            logger.info("üîÑ Ejecutando torch2trt en CPU...")
            try:
                self.model_trt = torch2trt.torch2trt(
                    self.model,
                    [self.test_input],
                    **conversion_params
                )
                conversion_active = False
                
                elapsed = time.time() - start_time
                logger.info("‚úÖ Conversi√≥n CPU completada en %.1f minutos", elapsed/60)
                
            except Exception as conversion_error:
                conversion_active = False
                raise conversion_error
            
            # Verificar modelo en CPU
            logger.info("üß™ Probando modelo TensorRT en CPU...")
            with torch.no_grad():
                trt_output = self.model_trt(self.test_input)
                logger.info("‚úÖ Inferencia TensorRT CPU exitosa")
            
            # Mover modelo final de vuelta a GPU para guardado
            logger.info("üì• Moviendo modelo convertido a GPU para guardado...")
            try:
                self.model_trt = self.model_trt.cuda()
                logger.info("‚úÖ Modelo movido a GPU exitosamente")
            except Exception as e:
                logger.warning("‚ö†Ô∏è No se pudo mover a GPU, guardando en CPU: %s", str(e))
            
            return True
            
        except Exception as e:
            logger.error("‚ùå Error durante conversi√≥n CPU: %s", str(e))
            return False
            
    def save_tensorrt_model(self):
        """Guarda el modelo TensorRT convertido"""
        logger.info("üíæ Guardando modelo TensorRT...")
        
        try:
            output_path = self.model_config['output_model']
            backup_path = self.model_config['backup_model']
            
            # Crear backup si ya existe
            if os.path.exists(output_path):
                logger.info("üìã Creando backup del modelo anterior...")
                import shutil
                shutil.copy2(output_path, backup_path)
                
            # Guardar nuevo modelo
            torch.save(self.model_trt.state_dict(), output_path)
            
            # Verificar archivo guardado
            if os.path.exists(output_path):
                model_size = os.path.getsize(output_path) / (1024**2)
                logger.info("‚úÖ Modelo TensorRT guardado: %s (%.1f MB)", output_path, model_size)
                return True
            else:
                logger.error("‚ùå Error: archivo no se guard√≥ correctamente")
                return False
                
        except Exception as e:
            logger.error("‚ùå Error guardando modelo: %s", str(e))
            return False
            
    def verify_converted_model(self):
        """Verifica que el modelo convertido funciona correctamente"""
        logger.info("üîç Verificando modelo convertido...")
        
        try:
            from torch2trt import TRTModule
            
            # Cargar modelo guardado
            model_verify = TRTModule()
            model_verify.load_state_dict(torch.load(self.model_config['output_model']))
            
            # Probar inferencia
            with torch.no_grad():
                test_output = model_verify(self.test_input)
                
            logger.info("‚úÖ Modelo TensorRT verificado correctamente")
            
            # Comparar rendimiento
            logger.info("‚ö° Comparando rendimiento...")
            
            # Benchmark PyTorch
            start_time = time.time()
            for _ in range(10):
                with torch.no_grad():
                    _ = self.model(self.test_input)
            pytorch_time = (time.time() - start_time) / 10
            
            # Benchmark TensorRT
            start_time = time.time()
            for _ in range(10):
                with torch.no_grad():
                    _ = model_verify(self.test_input)
            tensorrt_time = (time.time() - start_time) / 10
            
            speedup = pytorch_time / tensorrt_time if tensorrt_time > 0 else 1
            
            logger.info("üìä Resultados de rendimiento:")
            logger.info("   PyTorch: %.1f ms por inferencia", pytorch_time * 1000)
            logger.info("   TensorRT: %.1f ms por inferencia", tensorrt_time * 1000)
            logger.info("   Aceleraci√≥n: %.1fx", speedup)
            
            return True
            
        except Exception as e:
            logger.error("‚ùå Error verificando modelo: %s", str(e))
            return False
            
    def cleanup_resources(self):
        """Limpieza final de recursos"""
        logger.info("üßπ Limpiando recursos...")
        
        try:
            # Limpiar modelos de memoria
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'model_trt'):
                del self.model_trt
            if hasattr(self, 'test_input'):
                del self.test_input
                
            # Limpieza agresiva de memoria
            self._emergency_memory_cleanup()
            
            # Detener monitoreo
            self.resource_monitor.stop_monitoring()
            
            # Restaurar CPU
            self.cpu_limiter.restore_cpu_affinity()
            
            logger.info("‚úÖ Limpieza completada")
            
        except Exception as e:
            logger.error("Error durante limpieza: %s", str(e))
            
    def run_conversion(self):
        """Ejecuta el proceso completo de conversi√≥n"""
        logger.info("üöÄ Iniciando conversi√≥n completa PyTorch ‚Üí TensorRT")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. Configurar entorno
            if not self.setup_environment():
                return False
                
            # 2. Verificar archivos
            if not self.verify_model_files():
                return False
                
            # 3. Cargar configuraci√≥n
            if not self.load_model_configuration():
                return False
                
            # 4. Crear modelo PyTorch
            if not self.create_pytorch_model():
                return False
                
            # 5. Crear entrada de prueba
            if not self.create_test_input():
                return False
                
            # 6. Conversi√≥n a TensorRT
            if not self.convert_to_tensorrt():
                return False
                
            # 7. Guardar modelo
            if not self.save_tensorrt_model():
                return False
                
            # 8. Verificar modelo
            if not self.verify_converted_model():
                return False
                
            # Estad√≠sticas finales
            total_time = time.time() - start_time
            final_stats = self.resource_monitor.get_stats_summary()
            
            logger.info("üéâ ¬°Conversi√≥n completada exitosamente!")
            logger.info("=" * 60)
            logger.info("‚è±Ô∏è Tiempo total: %.1f minutos", total_time / 60)
            logger.info("üìÅ Modelo TensorRT: %s", self.model_config['output_model'])
            
            if final_stats:
                logger.info("üìä Estad√≠sticas de recursos:")
                logger.info("   CPU promedio: %.1f%%", final_stats['cpu']['average'])
                logger.info("   Memoria m√°xima: %.1f%%", final_stats['memory']['max'])
                if final_stats['temperature']['max']:
                    logger.info("   Temperatura m√°xima: %.1f¬∞C", final_stats['temperature']['max'])
                    
            return True
            
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è Conversi√≥n interrumpida por usuario")
            return False
        except Exception as e:
            logger.error("‚ùå Error cr√≠tico durante conversi√≥n: %s", str(e))
            return False
        finally:
            self.cleanup_resources()
    
    def _setup_aggressive_memory_management(self):
        """Configuraci√≥n agresiva de memoria para Jetson Nano con poca RAM"""
        logger.info("üõ†Ô∏è Configurando gesti√≥n agresiva de memoria...")
        
        # Variables de entorno para optimizaci√≥n de memoria
        memory_env = {
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:16,garbage_collection_threshold:0.6',
            'CUDA_LAUNCH_BLOCKING': '1',  # Sincronizaci√≥n para evitar acumulaci√≥n
            'TF_GPU_ALLOCATOR': 'cuda_malloc_async',  # Allocator optimizado
            'TRT_LOGGER_VERBOSITY': '0',  # Reducir verbosidad para ahorrar memoria
            'CUBLAS_WORKSPACE_CONFIG': ':16:8',  # Limitar workspace de cuBLAS
        }
        
        for key, value in memory_env.items():
            os.environ[key] = value
            logger.info(f"‚úÖ {key}={value}")
            
        # Configurar PyTorch para uso m√≠nimo de memoria
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = False  # Evitar caching de kernels
            torch.backends.cudnn.deterministic = True
            
            # Limitar memoria CUDA disponible
            available_ram_mb = psutil.virtual_memory().available / (1024**2)
            # Usar m√°ximo 70% de RAM disponible para CUDA
            cuda_memory_limit = min(int(available_ram_mb * 0.7), 1024)  # Max 1GB
            
            try:
                torch.cuda.set_per_process_memory_fraction(cuda_memory_limit / 1024.0)
                logger.info(f"‚úÖ L√≠mite CUDA: {cuda_memory_limit} MB")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo limitar memoria CUDA: {e}")
                
    def _pre_conversion_memory_optimization(self):
        """Optimizaciones de memoria antes de iniciar conversi√≥n"""
        logger.info("üßπ Optimizaci√≥n de memoria pre-conversi√≥n...")
        
        # Limpiar cach√©s del sistema
        try:
            subprocess.run(['sudo', 'sync'], check=False)
            subprocess.run(['sudo', 'sh', '-c', 'echo 3 > /proc/sys/vm/drop_caches'], check=False)
            logger.info("‚úÖ Cach√©s del sistema liberados")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudieron liberar cach√©s del sistema: {e}")
            
        # Garbage collection agresivo
        for _ in range(3):
            gc.collect()
            
        # Limpieza CUDA agresiva
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.reset_peak_memory_stats()
            
        # Verificar memoria despu√©s de limpieza
        memory = psutil.virtual_memory()
        logger.info(f"üíæ Memoria disponible tras limpieza: {memory.available / (1024**2):.0f} MB")
        
    def _adaptive_workspace_size(self):
        """Calcula el workspace size √≥ptimo basado en memoria disponible"""
        available_ram_mb = psutil.virtual_memory().available / (1024**2)
        
        # Usar m√°ximo 1% de RAM disponible para workspace, con l√≠mites
        workspace_mb = max(1, min(4, int(available_ram_mb * 0.01)))
        workspace_bytes = workspace_mb * 1024 * 1024
        
        logger.info(f"üéØ Workspace calculado: {workspace_mb} MB ({available_ram_mb:.0f} MB disponibles)")
        
        # Actualizar configuraci√≥n
        self.conversion_config['max_workspace_size'] = workspace_bytes
        return workspace_bytes
        
    def _validate_memory_requirements(self):
        """Valida si hay suficiente memoria para la conversi√≥n"""
        memory = psutil.virtual_memory()
        available_mb = memory.available / (1024**2)
        
        # Estimar memoria necesaria
        model_size_mb = 50  # ResNet18 base ~50MB
        conversion_overhead = 4  # Factor de overhead durante conversi√≥n
        minimum_required_mb = model_size_mb * conversion_overhead
        
        logger.info(f"üìä An√°lisis de memoria:")
        logger.info(f"   Disponible: {available_mb:.0f} MB")
        logger.info(f"   Requerido estimado: {minimum_required_mb:.0f} MB")
        
        if available_mb < minimum_required_mb:
            logger.error(f"‚ùå Memoria insuficiente: {available_mb:.0f} < {minimum_required_mb:.0f} MB")
            return False
            
        # Verificar swap si est√° disponible
        swap = psutil.swap_memory()
        if swap.total > 0:
            total_virtual_mb = available_mb + (swap.free / (1024**2))
            logger.info(f"   Total virtual (RAM+Swap): {total_virtual_mb:.0f} MB")
            
        return True
        
    def _create_model_with_memory_monitoring(self):
        """Crea modelo con monitoreo intensivo de memoria"""
        logger.info("üèóÔ∏è Creando modelo con monitoreo de memoria...")
        
        # Monitorear memoria antes
        initial_memory = psutil.virtual_memory()
        logger.info(f"üíæ Memoria inicial: {initial_memory.available / (1024**2):.0f} MB disponible")
        
        try:
            # Crear modelo paso a paso para detectar problemas de memoria
            logger.info("üìê Creando arquitectura del modelo...")
            self.model = trt_pose.models.resnet18_baseline_att(
                self.num_parts, 2 * self.num_links
            )
            
            # Verificar memoria despu√©s de crear arquitectura
            arch_memory = psutil.virtual_memory()
            arch_used = (initial_memory.available - arch_memory.available) / (1024**2)
            logger.info(f"üíæ Memoria usada por arquitectura: {arch_used:.1f} MB")
            
            # Mover a GPU con monitoreo
            logger.info("üéÆ Moviendo modelo a GPU...")
            self.model = self.model.cuda()
            
            gpu_memory = psutil.virtual_memory()
            gpu_used = (arch_memory.available - gpu_memory.available) / (1024**2)
            logger.info(f"üíæ Memoria usada por GPU transfer: {gpu_used:.1f} MB")
            
            # Poner en modo evaluaci√≥n
            self.model.eval()
            
            # Cargar pesos con monitoreo
            logger.info("üì• Cargando pesos...")
            checkpoint = torch.load(
                self.model_config['pytorch_model'], 
                map_location='cuda:0',
                weights_only=True  # M√°s seguro y eficiente
            )
            
            self.model.load_state_dict(checkpoint)
            del checkpoint  # Liberar inmediatamente
            gc.collect()
            
            # Verificar memoria final
            final_memory = psutil.virtual_memory()
            total_used = (initial_memory.available - final_memory.available) / (1024**2)
            logger.info(f"üíæ Memoria total usada por modelo: {total_used:.1f} MB")
            logger.info(f"üíæ Memoria restante: {final_memory.available / (1024**2):.0f} MB")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error creando modelo: {e}")
            # Limpieza en caso de error
            if hasattr(self, 'model'):
                del self.model
            torch.cuda.empty_cache()
            gc.collect()
            return False
